{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee5321d-d279-4bc6-8285-57a5d4a58ef1",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "201fb057-6645-4047-99a3-42756338b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TFBertLMHeadModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60530a72-4070-4ed2-a5a7-6627d9a9fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether torch is working\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5288e5b4-2378-45f5-99c3-b5f3b4db8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b6393-34e0-48fe-b75d-5f073650d8d0",
   "metadata": {},
   "source": [
    "## Preparing the dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397490d-7d4d-42a5-a82c-6cd72e614405",
   "metadata": {},
   "source": [
    "Provide a Json Lines (Jsonl) file containing question and answer pairs; like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b931e06-93db-4b56-ba16-2c7856d2fd72",
   "metadata": {},
   "source": [
    "{\"question\": \"what could be a use case of LLM for a hospital\", \"answer\": \"LLMs can have several applications in a hospital such as Assisstant for nurses\"}\n",
    "\n",
    "{\"question\": \"How AI is being used in medical industries?\", \"answer\":\" AI is being used in medical industries for various purposes such as disease diagnose\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4fd5967a-d5e8-4f06-b69c-85ef6d30a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"ds.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a9d54fa-8ad0-4f34-a322-ae4341354eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_df = pd.read_json(\"ds.jsonl\", lines=True)\n",
    "# dict_dataset = df.to_dict()\n",
    "# print(\"dataset contains \" + str(len(dict_dataset['question'])) + \" Q & A\")\n",
    "# dict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "748a7375-d62e-4cf3-8390-ec0c5e4e8be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_length = 2048\n",
    "# max_length = min(\n",
    "#     tokenized_inputs[\"input_ids\"].shape[1],\n",
    "#     max_length,\n",
    "# )\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6f2f1e52-849b-4c30-9ac4-5cf2f9b07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dict_dataset):\n",
    "    if \"question\" in dict_dataset and \"answer\" in dict_dataset:\n",
    "      text = dict_dataset[\"question\"][0] + dict_dataset[\"answer\"][0]\n",
    "    elif \"input\" in dict_dataset and \"output\" in dict_dataset:\n",
    "      text = dict_dataset[\"input\"][0] + dict_dataset[\"output\"][0]\n",
    "    else:\n",
    "      text = dict_dataset[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "55bf2fb1-0669-414e-97d7-d4daaa48ac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ff6950b1-f8dc-436a-b601-12867dbfb55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d3d55b67-4eaf-4872-87d2-b155150b235e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\",\n",
       " 'answer': 'Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.',\n",
       " 'input_ids': [1276,\n",
       "  403,\n",
       "  253,\n",
       "  1027,\n",
       "  3510,\n",
       "  273,\n",
       "  7177,\n",
       "  2130,\n",
       "  275,\n",
       "  253,\n",
       "  18491,\n",
       "  313,\n",
       "  70,\n",
       "  15,\n",
       "  72,\n",
       "  904,\n",
       "  12692,\n",
       "  7102,\n",
       "  13,\n",
       "  8990,\n",
       "  10097,\n",
       "  13,\n",
       "  13722,\n",
       "  434,\n",
       "  7102,\n",
       "  6177,\n",
       "  45,\n",
       "  4988,\n",
       "  74,\n",
       "  556,\n",
       "  10097,\n",
       "  327,\n",
       "  27669,\n",
       "  11075,\n",
       "  264,\n",
       "  13,\n",
       "  5271,\n",
       "  23058,\n",
       "  13,\n",
       "  19782,\n",
       "  37741,\n",
       "  10031,\n",
       "  13,\n",
       "  13814,\n",
       "  11397,\n",
       "  13,\n",
       "  378,\n",
       "  16464,\n",
       "  13,\n",
       "  11759,\n",
       "  10535,\n",
       "  1981,\n",
       "  13,\n",
       "  21798,\n",
       "  12989,\n",
       "  13,\n",
       "  285,\n",
       "  966,\n",
       "  10097,\n",
       "  327,\n",
       "  21708,\n",
       "  46,\n",
       "  10797,\n",
       "  2130,\n",
       "  387,\n",
       "  5987,\n",
       "  1358,\n",
       "  77,\n",
       "  4988,\n",
       "  74,\n",
       "  14,\n",
       "  2284,\n",
       "  15,\n",
       "  7280,\n",
       "  15,\n",
       "  900,\n",
       "  14206],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [1276,\n",
       "  403,\n",
       "  253,\n",
       "  1027,\n",
       "  3510,\n",
       "  273,\n",
       "  7177,\n",
       "  2130,\n",
       "  275,\n",
       "  253,\n",
       "  18491,\n",
       "  313,\n",
       "  70,\n",
       "  15,\n",
       "  72,\n",
       "  904,\n",
       "  12692,\n",
       "  7102,\n",
       "  13,\n",
       "  8990,\n",
       "  10097,\n",
       "  13,\n",
       "  13722,\n",
       "  434,\n",
       "  7102,\n",
       "  6177,\n",
       "  45,\n",
       "  4988,\n",
       "  74,\n",
       "  556,\n",
       "  10097,\n",
       "  327,\n",
       "  27669,\n",
       "  11075,\n",
       "  264,\n",
       "  13,\n",
       "  5271,\n",
       "  23058,\n",
       "  13,\n",
       "  19782,\n",
       "  37741,\n",
       "  10031,\n",
       "  13,\n",
       "  13814,\n",
       "  11397,\n",
       "  13,\n",
       "  378,\n",
       "  16464,\n",
       "  13,\n",
       "  11759,\n",
       "  10535,\n",
       "  1981,\n",
       "  13,\n",
       "  21798,\n",
       "  12989,\n",
       "  13,\n",
       "  285,\n",
       "  966,\n",
       "  10097,\n",
       "  327,\n",
       "  21708,\n",
       "  46,\n",
       "  10797,\n",
       "  2130,\n",
       "  387,\n",
       "  5987,\n",
       "  1358,\n",
       "  77,\n",
       "  4988,\n",
       "  74,\n",
       "  14,\n",
       "  2284,\n",
       "  15,\n",
       "  7280,\n",
       "  15,\n",
       "  900,\n",
       "  14206]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7a7027c0-8b71-4afa-bde6-9dcca5d69d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0743ee6c-d84a-4dc3-bd75-b24e1b56f51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      " Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Testing Dataset:\n",
      " Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]\n",
    "print(\"Training Dataset:\\n\", train_ds)\n",
    "print(\"Testing Dataset:\\n\", test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6daf1-6b9d-4ff6-97d6-bed26477268f",
   "metadata": {},
   "source": [
    "\n",
    "## Set up the model, training config, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "515c2aae-c1f3-4242-8725-e9c196a0f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6b8f6cbdf6427d83a3424a87f5fdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging into HuggingFace account\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "# my_token:  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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca4b24-3f90-4780-a80c-82da6a71b348",
   "metadata": {},
   "source": [
    "### PyTorch Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9e15c5f1-bb7d-4f2b-bad8-532f153ce64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017bed8-9c97-4c39-b820-fb68303fa198",
   "metadata": {},
   "source": [
    "#### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6638142d-7047-4b9b-8631-9cae5830fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ecb31478-ba86-4afa-aa10-e356eac1ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f\"{model_name}_{max_steps}_steps_{date.today()}-{datetime.now().strftime('%H:%M')}\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091a98f-4d69-412c-94d4-2dffcd36b217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc70c0c9-7a0e-425d-8e3a-0165b8bb6bbe",
   "metadata": {},
   "source": [
    "### Tensorflow Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d600ad49-45fd-45bf-9b96-2be125563850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `TFBertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "All PyTorch model weights were used when initializing TFBertLMHeadModel.\n",
      "\n",
      "All the weights of TFBertLMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# It may need case-wise considerations:\n",
    "if model_name == \"bert-base-uncased\":\n",
    "    model = TFBertLMHeadModel.from_pretrained(\"bert-base-uncased\")\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ead91fcc-8357-4bcf-a5cd-2f284f549558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13498db1-1729-427b-8fa4-8b2a929639d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359df9f3-beb0-4ff0-aca1-abe532ecedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfe00b-b642-457f-8171-87a154db1119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371827f-41cf-454a-9c48-04baa44aa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
