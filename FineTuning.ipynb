{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee5321d-d279-4bc6-8285-57a5d4a58ef1",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201fb057-6645-4047-99a3-42756338b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TFBertLMHeadModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60530a72-4070-4ed2-a5a7-6627d9a9fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether torch is working\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630eb89f-e4eb-4740-ac1a-ed60468431b0",
   "metadata": {},
   "source": [
    "\n",
    "#### If you want to download a model or a tokenizer from Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69536bf-3741-4516-b1a5-5ca35368de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging into HuggingFace account\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "# my_token:  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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa4840-bca9-43b3-8ed6-a2b47ed3c7a7",
   "metadata": {},
   "source": [
    "## Model and Tokenizer declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aeca32d8-7660-4922-9f88-8ef123c60448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bert-base-uncased\"\n",
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3b0004e-0d84-44bb-8057-fb6aa38f96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TFBertLMHeadModel.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eb94bb0-70ee-44dc-bfc1-74f826973c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c73bf69-160f-49d3-b1f9-192a2d45a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token    # for Eleuther model use this\n",
    "# tokenizer.pad_token = tokenizer.cls_token    # for Bert model use this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b6393-34e0-48fe-b75d-5f073650d8d0",
   "metadata": {},
   "source": [
    "## Preparing the dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397490d-7d4d-42a5-a82c-6cd72e614405",
   "metadata": {},
   "source": [
    "Provide a Json Lines (Jsonl) file containing question and answer pairs; like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b931e06-93db-4b56-ba16-2c7856d2fd72",
   "metadata": {},
   "source": [
    "{\"question\": \"what could be a use case of LLM for a hospital\", \"answer\": \"LLMs can have several applications in a hospital such as Assisstant for nurses\"}\n",
    "\n",
    "{\"question\": \"How AI is being used in medical industries?\", \"answer\":\" AI is being used in medical industries for various purposes such as disease diagnose\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4fd5967a-d5e8-4f06-b69c-85ef6d30a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"ds.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a9d54fa-8ad0-4f34-a322-ae4341354eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_df = pd.read_json(\"ds.jsonl\", lines=True)\n",
    "# dict_dataset = df.to_dict()\n",
    "# print(\"dataset contains \" + str(len(dict_dataset['question'])) + \" Q & A\")\n",
    "# dict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34ea013b-5432-4d82-a946-5d8e7741e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding and truncating mey be used if model input exceeds max_length\n",
    "max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f2f1e52-849b-4c30-9ac4-5cf2f9b07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dict_dataset):\n",
    "    if \"question\" in dict_dataset and \"answer\" in dict_dataset:\n",
    "      text = dict_dataset[\"question\"][0] + dict_dataset[\"answer\"][0]\n",
    "    elif \"input\" in dict_dataset and \"output\" in dict_dataset:\n",
    "      text = dict_dataset[\"input\"][0] + dict_dataset[\"output\"][0]\n",
    "    else:\n",
    "      text = dict_dataset[\"text\"][0]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55bf2fb1-0669-414e-97d7-d4daaa48ac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e874dac848472494c5f8de55db8661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff6950b1-f8dc-436a-b601-12867dbfb55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3d55b67-4eaf-4872-87d2-b155150b235e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\",\n",
       " 'answer': 'Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.',\n",
       " 'input_ids': [1276,\n",
       "  403,\n",
       "  253,\n",
       "  1027,\n",
       "  3510,\n",
       "  273,\n",
       "  7177,\n",
       "  2130,\n",
       "  275,\n",
       "  253,\n",
       "  18491,\n",
       "  313,\n",
       "  70,\n",
       "  15,\n",
       "  72,\n",
       "  904,\n",
       "  12692,\n",
       "  7102,\n",
       "  13,\n",
       "  8990,\n",
       "  10097,\n",
       "  13,\n",
       "  13722,\n",
       "  434,\n",
       "  7102,\n",
       "  6177,\n",
       "  45,\n",
       "  4988,\n",
       "  74,\n",
       "  556,\n",
       "  10097,\n",
       "  327,\n",
       "  27669,\n",
       "  11075,\n",
       "  264,\n",
       "  13,\n",
       "  5271,\n",
       "  23058,\n",
       "  13,\n",
       "  19782,\n",
       "  37741,\n",
       "  10031,\n",
       "  13,\n",
       "  13814,\n",
       "  11397,\n",
       "  13,\n",
       "  378,\n",
       "  16464,\n",
       "  13,\n",
       "  11759,\n",
       "  10535,\n",
       "  1981,\n",
       "  13,\n",
       "  21798,\n",
       "  12989,\n",
       "  13,\n",
       "  285,\n",
       "  966,\n",
       "  10097,\n",
       "  327,\n",
       "  21708,\n",
       "  46,\n",
       "  10797,\n",
       "  2130,\n",
       "  387,\n",
       "  5987,\n",
       "  1358,\n",
       "  77,\n",
       "  4988,\n",
       "  74,\n",
       "  14,\n",
       "  2284,\n",
       "  15,\n",
       "  7280,\n",
       "  15,\n",
       "  900,\n",
       "  14206],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [1276,\n",
       "  403,\n",
       "  253,\n",
       "  1027,\n",
       "  3510,\n",
       "  273,\n",
       "  7177,\n",
       "  2130,\n",
       "  275,\n",
       "  253,\n",
       "  18491,\n",
       "  313,\n",
       "  70,\n",
       "  15,\n",
       "  72,\n",
       "  904,\n",
       "  12692,\n",
       "  7102,\n",
       "  13,\n",
       "  8990,\n",
       "  10097,\n",
       "  13,\n",
       "  13722,\n",
       "  434,\n",
       "  7102,\n",
       "  6177,\n",
       "  45,\n",
       "  4988,\n",
       "  74,\n",
       "  556,\n",
       "  10097,\n",
       "  327,\n",
       "  27669,\n",
       "  11075,\n",
       "  264,\n",
       "  13,\n",
       "  5271,\n",
       "  23058,\n",
       "  13,\n",
       "  19782,\n",
       "  37741,\n",
       "  10031,\n",
       "  13,\n",
       "  13814,\n",
       "  11397,\n",
       "  13,\n",
       "  378,\n",
       "  16464,\n",
       "  13,\n",
       "  11759,\n",
       "  10535,\n",
       "  1981,\n",
       "  13,\n",
       "  21798,\n",
       "  12989,\n",
       "  13,\n",
       "  285,\n",
       "  966,\n",
       "  10097,\n",
       "  327,\n",
       "  21708,\n",
       "  46,\n",
       "  10797,\n",
       "  2130,\n",
       "  387,\n",
       "  5987,\n",
       "  1358,\n",
       "  77,\n",
       "  4988,\n",
       "  74,\n",
       "  14,\n",
       "  2284,\n",
       "  15,\n",
       "  7280,\n",
       "  15,\n",
       "  900,\n",
       "  14206]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a7027c0-8b71-4afa-bde6-9dcca5d69d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0743ee6c-d84a-4dc3-bd75-b24e1b56f51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      " Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Testing Dataset:\n",
      " Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]\n",
    "print(\"Training Dataset:\\n\", train_ds)\n",
    "print(\"Testing Dataset:\\n\", test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca4b24-3f90-4780-a80c-82da6a71b348",
   "metadata": {},
   "source": [
    "### PyTorch Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9f540923-810d-4d5a-8939-79d44741711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class to include logging and history\n",
    "class Trainer(transformers.Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        model_flops,\n",
    "        total_steps,\n",
    "        args=None,\n",
    "        data_collator=None,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=None,\n",
    "        tokenizer=None,\n",
    "        model_init=None,\n",
    "        compute_metrics=None,\n",
    "        callbacks=None,\n",
    "        optimizers=(None, None),\n",
    "    ):\n",
    "        super(Trainer, self).__init__(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            tokenizer,\n",
    "            model_init,\n",
    "            compute_metrics,\n",
    "            callbacks,\n",
    "            optimizers,\n",
    "        )\n",
    "\n",
    "        self.total_steps = total_steps\n",
    "        self.model_flops = model_flops\n",
    "        self.start_step = 0\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        if inputs[\"input_ids\"].numel() == 0:\n",
    "\n",
    "          print(\"Inputs: \", inputs)\n",
    "          print(\"Inputs - input_ids\", inputs[\"input_ids\"])\n",
    "          print(\"numel\", inputs[\"input_ids\"].numel())\n",
    "\n",
    "          return torch.tensor(0)\n",
    "        else:\n",
    "          model.train()\n",
    "          inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "          with self.compute_loss_context_manager():\n",
    "              loss = self.compute_loss(model, inputs)\n",
    "\n",
    "          if self.args.n_gpu > 1:\n",
    "              loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "          if self.do_grad_scaling:\n",
    "              self.scaler.scale(loss).backward()\n",
    "          else:\n",
    "              self.accelerator.backward(loss)\n",
    "\n",
    "          return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def log(self, logs):\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        self.update_log_timing(logs)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.update_history(output)\n",
    "\n",
    "        logger.debug(\"Step (\" + str(self.state.global_step) + \") Logs: \" + str(logs))\n",
    "        self.control = self.callback_handler.on_log(\n",
    "            self.args, self.state, self.control, logs\n",
    "        )\n",
    "\n",
    "    def update_log_timing(self, logs):\n",
    "        if len(self.state.log_history) == 0:\n",
    "            self.start_time = time.time()\n",
    "            logs[\"iter_time\"] = 0.0\n",
    "            logs[\"flops\"] = 0.0\n",
    "            logs[\"remaining_time\"] = 0.0\n",
    "            self.start_step = self.state.global_step\n",
    "        elif self.state.global_step > self.start_step:\n",
    "            logs[\"iter_time\"] = (time.time() - self.start_time) / (\n",
    "                self.state.global_step - self.start_step\n",
    "            )\n",
    "            logs[\"flops\"] = self.model_flops / logs[\"iter_time\"]\n",
    "            logs[\"remaining_time\"] = (self.total_steps - self.state.global_step) * logs[\n",
    "                \"iter_time\"\n",
    "            ]\n",
    "\n",
    "    def update_history(self, output):\n",
    "        if \"eval_loss\" in output:\n",
    "            return\n",
    "        if len(self.state.log_history) > 0:\n",
    "            smoothing_window = 100\n",
    "            p = 1.0 / smoothing_window\n",
    "            if \"loss\" in output:\n",
    "                output[\"loss\"] = output[\"loss\"] * p + self.state.log_history[-1][\n",
    "                    \"loss\"\n",
    "                ] * (1.0 - p)\n",
    "        self.state.log_history.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e15c5f1-bb7d-4f2b-bad8-532f153ce64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "96ca5b54-1189-4f3c-bc90-99d50cb5a134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d81c79b3-815b-4770-aff5-75f5168c6fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I get the correct documentation to work?\n",
      "\n",
      "A:\n",
      "\n",
      "I think you need to use the following code:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following\n"
     ]
    }
   ],
   "source": [
    "test_text = test_ds[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_ds[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017bed8-9c97-4c39-b820-fb68303fa198",
   "metadata": {},
   "source": [
    "#### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6638142d-7047-4b9b-8631-9cae5830fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ecb31478-ba86-4afa-aa10-e356eac1ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f\"{model_name}_{max_steps}_steps_{date.today()}--{datetime.now().strftime('%H;%M')}\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c091a98f-4d69-412c-94d4-2dffcd36b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=20,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a133d89a-df73-4f4b-9e35-6a7a15491571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.30687256 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, max_length)\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(\"Memory footprint\", model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "56161875-5859-4e3d-9dac-ba84513c6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ea7f2eef-a0a4-43e0-8f12-c4ce1e91a971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 02:54, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.785011</td>\n",
       "      <td>2.336461</td>\n",
       "      <td>142.115101</td>\n",
       "      <td>13595934958324.591797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.432526</td>\n",
       "      <td>2.198434</td>\n",
       "      <td>125.020408</td>\n",
       "      <td>13347481141061.873047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.164942</td>\n",
       "      <td>2.149117</td>\n",
       "      <td>106.818674</td>\n",
       "      <td>13155259694826.115234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.983753</td>\n",
       "      <td>2.108783</td>\n",
       "      <td>87.745057</td>\n",
       "      <td>13012097808168.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.899057</td>\n",
       "      <td>2.085440</td>\n",
       "      <td>68.563866</td>\n",
       "      <td>12809474871045.728516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.775090</td>\n",
       "      <td>2.083733</td>\n",
       "      <td>47.537109</td>\n",
       "      <td>12932780348357.845703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.708021</td>\n",
       "      <td>2.063934</td>\n",
       "      <td>27.176818</td>\n",
       "      <td>12926710406766.826172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.633429</td>\n",
       "      <td>2.059559</td>\n",
       "      <td>6.868385</td>\n",
       "      <td>12787098297236.974609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n",
      "Inputs:  {'input_ids': tensor([], device='cuda:0', size=(1, 0)), 'attention_mask': tensor([], device='cuda:0', size=(1, 0)), 'labels': tensor([], device='cuda:0', size=(1, 0))}\n",
      "Inputs - input_ids tensor([], device='cuda:0', size=(1, 0))\n",
      "numel 0\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "86d5b647-f1ad-4aea-9b13-7923eddfbcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: EleutherAI/pythia-70m_1000_steps_2023-08-30--08;16/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d082e27b-a977-4f8d-8099-0f9989cc59c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "finetuned_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d1463840-39a9-4a7b-b120-4af3bbe65e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Finetuned model's answer: \n",
      "Yes, Lamini can generate technical documentation or user manuals for software projects. Lamini can generate technical documentation or user manuals for software projects. It can also generate documentation or user-provided documentation for software projects. It can also generate documentation for software projects. It can also generate documentation for software projects. It can also generate documentation for software projects. It can also generate documentation for software projects. It can also\n"
     ]
    }
   ],
   "source": [
    "test_question = test_ds[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned model's answer: \")\n",
    "print(inference(test_question, finetuned_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70c0c9-7a0e-425d-8e3a-0165b8bb6bbe",
   "metadata": {},
   "source": [
    "### Tensorflow Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d600ad49-45fd-45bf-9b96-2be125563850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `TFBertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "All PyTorch model weights were used when initializing TFBertLMHeadModel.\n",
      "\n",
      "All the weights of TFBertLMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# It may need case-wise considerations:\n",
    "if model_name == \"bert-base-uncased\":\n",
    "    model = TFBertLMHeadModel.from_pretrained(\"bert-base-uncased\")\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ead91fcc-8357-4bcf-a5cd-2f284f549558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13498db1-1729-427b-8fa4-8b2a929639d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359df9f3-beb0-4ff0-aca1-abe532ecedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfe00b-b642-457f-8171-87a154db1119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371827f-41cf-454a-9c48-04baa44aa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
