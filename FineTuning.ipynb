{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee5321d-d279-4bc6-8285-57a5d4a58ef1",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "201fb057-6645-4047-99a3-42756338b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TFBertLMHeadModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "60530a72-4070-4ed2-a5a7-6627d9a9fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether torch is working\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630eb89f-e4eb-4740-ac1a-ed60468431b0",
   "metadata": {},
   "source": [
    "\n",
    "#### If you want to download a model or a tokenizer from Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c69536bf-3741-4516-b1a5-5ca35368de11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b45373ee124be4b969e1249794a49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging into HuggingFace account\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "# my_token:  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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa4840-bca9-43b3-8ed6-a2b47ed3c7a7",
   "metadata": {},
   "source": [
    "## Model and Tokenizer declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "aeca32d8-7660-4922-9f88-8ef123c60448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bert-base-uncased\"\n",
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "d3b0004e-0d84-44bb-8057-fb6aa38f96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TFBertLMHeadModel.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "5eb94bb0-70ee-44dc-bfc1-74f826973c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "1c73bf69-160f-49d3-b1f9-192a2d45a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token    # for Eleuther model use this\n",
    "# tokenizer.pad_token = tokenizer.cls_token    # for Bert model use this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b6393-34e0-48fe-b75d-5f073650d8d0",
   "metadata": {},
   "source": [
    "## Preparing the dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397490d-7d4d-42a5-a82c-6cd72e614405",
   "metadata": {},
   "source": [
    "Provide a Json Lines (Jsonl) file containing question and answer pairs; like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b931e06-93db-4b56-ba16-2c7856d2fd72",
   "metadata": {},
   "source": [
    "{\"question\": \"what could be a use case of LLM for a hospital\", \"answer\": \"LLMs can have several applications in a hospital such as Assisstant for nurses\"}\n",
    "\n",
    "{\"question\": \"How AI is being used in medical industries?\", \"answer\":\" AI is being used in medical industries for various purposes such as disease diagnose\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "4fd5967a-d5e8-4f06-b69c-85ef6d30a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"ds.jsonl\"\n",
    "dataset_path = \"ds-sat.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "3a9d54fa-8ad0-4f34-a322-ae4341354eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_df = pd.read_json(\"ds.jsonl\", lines=True)\n",
    "# dict_dataset = df.to_dict()\n",
    "# print(\"dataset contains \" + str(len(dict_dataset['question'])) + \" Q & A\")\n",
    "# dict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "34ea013b-5432-4d82-a946-5d8e7741e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding and truncating mey be used if model input exceeds max_length\n",
    "max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "6f2f1e52-849b-4c30-9ac4-5cf2f9b07f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dict_dataset):\n",
    "    if \"question\" in dict_dataset and \"answer\" in dict_dataset:\n",
    "      text = dict_dataset[\"question\"][0] + dict_dataset[\"answer\"][0]\n",
    "    elif \"input\" in dict_dataset and \"output\" in dict_dataset:\n",
    "      text = dict_dataset[\"input\"][0] + dict_dataset[\"output\"][0]\n",
    "    else:\n",
    "      text = dict_dataset[\"text\"][0]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "5b94b1ec-475d-4790-b52d-1207d7af6179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why is satellite imagery a valuable source of data for assessing maritime traffic??'"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuning_dataset_loaded[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "55bf2fb1-0669-414e-97d7-d4daaa48ac7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479a66b22093433fb0cf6d57df36e9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 45\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "ff6950b1-f8dc-436a-b601-12867dbfb55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "d3d55b67-4eaf-4872-87d2-b155150b235e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Why is satellite imagery a valuable source of data for assessing maritime traffic??',\n",
       " 'answer': 'Satellite imagery is a valuable source of data for assessing maritime traffic because it allows access to information that is difficult to obtain by other means. Satellites provide a global geographic coverage and collect information periodically with high temporal resolution. This means that satellite images can provide a comprehensive view of maritime traffic in a given region over time.',\n",
       " 'input_ids': [4967,\n",
       "  310,\n",
       "  15109,\n",
       "  27471,\n",
       "  247,\n",
       "  9865,\n",
       "  2603,\n",
       "  273,\n",
       "  941,\n",
       "  323,\n",
       "  18005,\n",
       "  37223,\n",
       "  7137,\n",
       "  8220,\n",
       "  20794,\n",
       "  29873,\n",
       "  27471,\n",
       "  310,\n",
       "  247,\n",
       "  9865,\n",
       "  2603,\n",
       "  273,\n",
       "  941,\n",
       "  323,\n",
       "  18005,\n",
       "  37223,\n",
       "  7137,\n",
       "  984,\n",
       "  352,\n",
       "  4483,\n",
       "  2289,\n",
       "  281,\n",
       "  1491,\n",
       "  326,\n",
       "  310,\n",
       "  2834,\n",
       "  281,\n",
       "  4044,\n",
       "  407,\n",
       "  643,\n",
       "  2097,\n",
       "  15,\n",
       "  11191,\n",
       "  437,\n",
       "  3254,\n",
       "  2085,\n",
       "  247,\n",
       "  4156,\n",
       "  23365,\n",
       "  7031,\n",
       "  285,\n",
       "  4822,\n",
       "  1491,\n",
       "  28557,\n",
       "  342,\n",
       "  1029,\n",
       "  11935,\n",
       "  6064,\n",
       "  15,\n",
       "  831,\n",
       "  2097,\n",
       "  326,\n",
       "  15109,\n",
       "  3888,\n",
       "  476,\n",
       "  2085,\n",
       "  247,\n",
       "  11088,\n",
       "  1859,\n",
       "  273,\n",
       "  37223,\n",
       "  7137,\n",
       "  275,\n",
       "  247,\n",
       "  1677,\n",
       "  2919,\n",
       "  689,\n",
       "  673,\n",
       "  15],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [4967,\n",
       "  310,\n",
       "  15109,\n",
       "  27471,\n",
       "  247,\n",
       "  9865,\n",
       "  2603,\n",
       "  273,\n",
       "  941,\n",
       "  323,\n",
       "  18005,\n",
       "  37223,\n",
       "  7137,\n",
       "  8220,\n",
       "  20794,\n",
       "  29873,\n",
       "  27471,\n",
       "  310,\n",
       "  247,\n",
       "  9865,\n",
       "  2603,\n",
       "  273,\n",
       "  941,\n",
       "  323,\n",
       "  18005,\n",
       "  37223,\n",
       "  7137,\n",
       "  984,\n",
       "  352,\n",
       "  4483,\n",
       "  2289,\n",
       "  281,\n",
       "  1491,\n",
       "  326,\n",
       "  310,\n",
       "  2834,\n",
       "  281,\n",
       "  4044,\n",
       "  407,\n",
       "  643,\n",
       "  2097,\n",
       "  15,\n",
       "  11191,\n",
       "  437,\n",
       "  3254,\n",
       "  2085,\n",
       "  247,\n",
       "  4156,\n",
       "  23365,\n",
       "  7031,\n",
       "  285,\n",
       "  4822,\n",
       "  1491,\n",
       "  28557,\n",
       "  342,\n",
       "  1029,\n",
       "  11935,\n",
       "  6064,\n",
       "  15,\n",
       "  831,\n",
       "  2097,\n",
       "  326,\n",
       "  15109,\n",
       "  3888,\n",
       "  476,\n",
       "  2085,\n",
       "  247,\n",
       "  11088,\n",
       "  1859,\n",
       "  273,\n",
       "  37223,\n",
       "  7137,\n",
       "  275,\n",
       "  247,\n",
       "  1677,\n",
       "  2919,\n",
       "  689,\n",
       "  673,\n",
       "  15]}"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "7a7027c0-8b71-4afa-bde6-9dcca5d69d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=14)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "0743ee6c-d84a-4dc3-bd75-b24e1b56f51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      " Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 40\n",
      "})\n",
      "Testing Dataset:\n",
      " Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]\n",
    "print(\"Training Dataset:\\n\", train_ds)\n",
    "print(\"Testing Dataset:\\n\", test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca4b24-3f90-4780-a80c-82da6a71b348",
   "metadata": {},
   "source": [
    "### PyTorch Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "9f540923-810d-4d5a-8939-79d44741711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class to include logging and history\n",
    "class Trainer(transformers.Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        model_flops,\n",
    "        total_steps,\n",
    "        args=None,\n",
    "        data_collator=None,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=None,\n",
    "        tokenizer=None,\n",
    "        model_init=None,\n",
    "        compute_metrics=None,\n",
    "        callbacks=None,\n",
    "        optimizers=(None, None),\n",
    "    ):\n",
    "        super(Trainer, self).__init__(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            tokenizer,\n",
    "            model_init,\n",
    "            compute_metrics,\n",
    "            callbacks,\n",
    "            optimizers,\n",
    "        )\n",
    "\n",
    "        self.total_steps = total_steps\n",
    "        self.model_flops = model_flops\n",
    "        self.start_step = 0\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        if inputs[\"input_ids\"].numel() == 0:\n",
    "\n",
    "          print(\"Inputs: \", inputs)\n",
    "          print(\"Inputs - input_ids\", inputs[\"input_ids\"])\n",
    "          print(\"numel\", inputs[\"input_ids\"].numel())\n",
    "\n",
    "          return torch.tensor(0)\n",
    "        else:\n",
    "          model.train()\n",
    "          inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "          with self.compute_loss_context_manager():\n",
    "              loss = self.compute_loss(model, inputs)\n",
    "\n",
    "          if self.args.n_gpu > 1:\n",
    "              loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "          if self.do_grad_scaling:\n",
    "              self.scaler.scale(loss).backward()\n",
    "          else:\n",
    "              self.accelerator.backward(loss)\n",
    "\n",
    "          return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def log(self, logs):\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        self.update_log_timing(logs)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.update_history(output)\n",
    "\n",
    "        logger.debug(\"Step (\" + str(self.state.global_step) + \") Logs: \" + str(logs))\n",
    "        self.control = self.callback_handler.on_log(\n",
    "            self.args, self.state, self.control, logs\n",
    "        )\n",
    "\n",
    "    def update_log_timing(self, logs):\n",
    "        if len(self.state.log_history) == 0:\n",
    "            self.start_time = time.time()\n",
    "            logs[\"iter_time\"] = 0.0\n",
    "            logs[\"flops\"] = 0.0\n",
    "            logs[\"remaining_time\"] = 0.0\n",
    "            self.start_step = self.state.global_step\n",
    "        elif self.state.global_step > self.start_step:\n",
    "            logs[\"iter_time\"] = (time.time() - self.start_time) / (\n",
    "                self.state.global_step - self.start_step\n",
    "            )\n",
    "            logs[\"flops\"] = self.model_flops / logs[\"iter_time\"]\n",
    "            logs[\"remaining_time\"] = (self.total_steps - self.state.global_step) * logs[\n",
    "                \"iter_time\"\n",
    "            ]\n",
    "\n",
    "    def update_history(self, output):\n",
    "        if \"eval_loss\" in output:\n",
    "            return\n",
    "        if len(self.state.log_history) > 0:\n",
    "            smoothing_window = 100\n",
    "            p = 1.0 / smoothing_window\n",
    "            if \"loss\" in output:\n",
    "                output[\"loss\"] = output[\"loss\"] * p + self.state.log_history[-1][\n",
    "                    \"loss\"\n",
    "                ] * (1.0 - p)\n",
    "        self.state.log_history.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "9e15c5f1-bb7d-4f2b-bad8-532f153ce64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "96ca5b54-1189-4f3c-bc90-99d50cb5a134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d81c79b3-815b-4770-aff5-75f5168c6fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): What is the significance of ship detection with satellite imagery?\n",
      "Correct answer from Lamini docs: Ship detection with satellite imagery has various applications, such as maritime surveillance, vessel traffic management, illegal fishing detection, and monitoring trade activities.\n",
      "Model's answer: \n",
      "\n",
      "\n",
      "The answer is that the satellite imagery is not a satellite imagery, but a satellite imagery. The satellite imagery is a satellite imagery of the moon, the moon, the moon, and the moon. The satellite imagery is a satellite imagery of the moon, the moon, and the moon. The satellite imagery is a satellite imagery of the moon, the moon, and the moon. The satellite imagery is a satellite imagery of the moon, the\n"
     ]
    }
   ],
   "source": [
    "test_text = test_ds[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_ds[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017bed8-9c97-4c39-b820-fb68303fa198",
   "metadata": {},
   "source": [
    "#### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "6638142d-7047-4b9b-8631-9cae5830fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "ecb31478-ba86-4afa-aa10-e356eac1ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model_name = f\"{model_name}_{max_steps}_steps_{date.today()}--{datetime.now().strftime('%H;%M')}\"\n",
    "trained_model_name = f\"{model_name}_{max_steps}_steps_Satellite-{date.today()}--{datetime.now().strftime('%H;%M')}\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c091a98f-4d69-412c-94d4-2dffcd36b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=2.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=30,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a133d89a-df73-4f4b-9e35-6a7a15491571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.30687256 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, max_length)\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(\"Memory footprint\", model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "56161875-5859-4e3d-9dac-ba84513c6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ea7f2eef-a0a4-43e0-8f12-c4ce1e91a971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:26, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.013942</td>\n",
       "      <td>4.997031</td>\n",
       "      <td>352.875124</td>\n",
       "      <td>5475556488160.918945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.381349</td>\n",
       "      <td>5.724802</td>\n",
       "      <td>300.313096</td>\n",
       "      <td>5556559329888.556641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.174673</td>\n",
       "      <td>6.009246</td>\n",
       "      <td>252.835046</td>\n",
       "      <td>5557882182907.407227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.105261</td>\n",
       "      <td>6.208739</td>\n",
       "      <td>204.001213</td>\n",
       "      <td>5596767033568.611328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.081003</td>\n",
       "      <td>6.297903</td>\n",
       "      <td>155.939001</td>\n",
       "      <td>5632119732029.251953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.071567</td>\n",
       "      <td>6.391783</td>\n",
       "      <td>108.683712</td>\n",
       "      <td>5656661681397.458984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.066342</td>\n",
       "      <td>6.495336</td>\n",
       "      <td>61.972798</td>\n",
       "      <td>5668726659740.708984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.062209</td>\n",
       "      <td>6.540971</td>\n",
       "      <td>15.458036</td>\n",
       "      <td>5681621531441.122070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "86d5b647-f1ad-4aea-9b13-7923eddfbcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: EleutherAI/pythia-70m_1000_steps_Satellite-2023-08-30--15;27/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "d082e27b-a977-4f8d-8099-0f9989cc59c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "finetuned_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "d1463840-39a9-4a7b-b120-4af3bbe65e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): What are some algorithms commonly used for sentiment segmentation of images?\n",
      "\n",
      "Answer from dataset (test):\n",
      " Some common algorithms used for sentiment segmentation of images include Mask R-CNN, DeepLab, and U-Net. These algorithms leverage deep learning and convolutional neural network techniques to analyze and classify different regions of an image based on their sentiment or emotional content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base model's answer:\n",
      " \n",
      "\n",
      "A:\n",
      "\n",
      "I think you should use a simple algorithm to get the best results.\n",
      "The algorithm is called the \"best\" algorithm.\n",
      "The algorithm is called the \"best\" algorithm.\n",
      "The algorithm is called the \"best\" algorithm.\n",
      "The algorithm is called the \"best\" algorithm.\n",
      "The algorithm is called the \"best\" algorithm.\n",
      "The algorithm is called the \"best\" algorithm.\n",
      "The\n",
      "\n",
      "Finetuned model's answer: \n",
      "Various algorithms can be used, including convolutional neural networks (CNNs), support vector machines (SVMs), random forests, or ensemble models, depending on the specific requirements and characteristics of the applied task. These algorithms can be used in various contexts, including sentiment segmentation, sentiment detection, and classification tasks, depending on the specific requirements and characteristics of the task. Additionally, some algorithms can perform better in some contexts, such as object detection\n"
     ]
    }
   ],
   "source": [
    "test_instance = 3\n",
    "test_question = test_ds[test_instance]['question']\n",
    "test_answer = test_ds[test_instance]['answer']\n",
    "print(\"Question input (test):\", test_question)\n",
    "print(\"\\nAnswer from dataset (test):\\n\", test_answer)\n",
    "print(\"\\nBase model's answer:\\n\",inference(test_question, base_model, tokenizer))\n",
    "print(\"\\nFinetuned model's answer: \")\n",
    "print(inference(test_question, finetuned_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "ec5df545-1003-4f87-a7f9-45b887045fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ship detection using satellite imagery is a process of automatically identifying and locating ships in satellite images using machine learning models and AI algorithms. It involves using machine learning models and AI algorithms to identify and locate ships in satellite images. This process can be challenging for ship detection because of the complexity of ship detection using satellite imagery. Additionally, ship detection using satellite imagery is not feasible for ship detection because of the low performance of SAR detection models. Additionally, ship detection using satellite\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"How are machine learning models trained for ship detection?\"\"\"\n",
    "print(inference(train_ds[5]['question'], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51783d0-5445-4fc4-b2f8-20fbe0f6e32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc70c0c9-7a0e-425d-8e3a-0165b8bb6bbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tensorflow Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600ad49-45fd-45bf-9b96-2be125563850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It may need case-wise considerations:\n",
    "if model_name == \"bert-base-uncased\":\n",
    "    model = TFBertLMHeadModel.from_pretrained(\"bert-base-uncased\")\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead91fcc-8357-4bcf-a5cd-2f284f549558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13498db1-1729-427b-8fa4-8b2a929639d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359df9f3-beb0-4ff0-aca1-abe532ecedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfe00b-b642-457f-8171-87a154db1119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371827f-41cf-454a-9c48-04baa44aa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
